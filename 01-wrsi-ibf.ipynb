{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907c8027-8b66-426a-bb3e-a1b8a55fc55d",
   "metadata": {},
   "source": [
    "# TAMSAT ALERT WRSI Drought IBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a424f4-911b-4016-b30d-f039fe7da52e",
   "metadata": {},
   "source": [
    "## impact function \n",
    "\n",
    "1. **Import Required Modules**:\n",
    "   - `ImpactFuncSet` from `climada.entity`: Used to handle and analyze impact functions.\n",
    "   - `ENT_TEMPLATE_XLS` from `climada.util`: Provides a default template for creating impact function Excel files.\n",
    "   - `matplotlib.pyplot`: Used for visualizing the impact functions.\n",
    "\n",
    "2. **Specify Input Excel File**:\n",
    "   ```python\n",
    "   file_name = '../../ibf_drought_impact_ea_v0.xlsx'\n",
    "   ```\n",
    "   - Specifies the path to the Excel file containing impact functions (e.g., drought impact functions for East Africa).\n",
    "\n",
    "3. **Load Impact Functions**:\n",
    "   ```python\n",
    "   imp_set_xlsx = ImpactFuncSet.from_excel(file_name)\n",
    "   ```\n",
    "   - Reads the impact functions from the specified Excel file into an `ImpactFuncSet` object (`imp_set_xlsx`).\n",
    "   - The file is expected to follow the CLIMADA impact function template format.\n",
    "\n",
    "4. **Plot Impact Functions**:\n",
    "   ```python\n",
    "   imp_set_xlsx.plot()\n",
    "   ```\n",
    "   - Visualizes the impact functions in the `ImpactFuncSet`.\n",
    "   - Each function is typically plotted with hazard intensity on the x-axis and damage fraction on the y-axis.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose:\n",
    "The code loads drought impact functions from an Excel file and plots them to visualize how hazard intensity (e.g., drought severity) affects the exposed entities (e.g., crops, infrastructure) in East Africa. This helps in understanding the relationship between hazard and impact for different scenarios.\n",
    "\n",
    "In CLIMADA, MDD, PAA, and MDR are key parameters used in defining impact functions, which describe the relationship between hazard intensity and the resulting damage or loss.\n",
    "\n",
    "MDD: Mean Damage Degree - The fraction of the exposed value expected to be lost at a given hazard intensity.\n",
    "PAA: Peak Absolute Area - The intensity at which the maximum damage occurs in the impact function.\n",
    "MDR: Maximum Damage Ratio - The upper limit of damage as a proportion of the exposed value, typically normalized to 1 (or 100%).\n",
    "These parameters collectively model how a hazard (e.g., windstorm, flood) impacts the exposure based on its intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6b63c-c6c7-4915-9b0c-d78107b5cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import ImpactFuncSet\n",
    "from climada.util import ENT_TEMPLATE_XLS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# provide absolute path of the input excel file\n",
    "#file_name = ENT_TEMPLATE_XLS\n",
    "file_name='ibf_drought_impact_ea_v0.xlsx'\n",
    "\n",
    "imp_set_xlsx = ImpactFuncSet.from_excel(file_name)\n",
    "\n",
    "imp_set_xlsx.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787737b-f605-4dd2-b312-7b43e202f282",
   "metadata": {},
   "source": [
    "## exposure \n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - Imports required libraries like `pandas`, `geopandas`, and `cartopy` for data manipulation and geospatial processing.\n",
    "\n",
    "2. **Read CSV File (`ea_agr_spam.csv`)**:\n",
    "   - Loads crop production statistics data into a pandas DataFrame (`ex_db`).\n",
    "\n",
    "3. **Chunk the Data**:\n",
    "   - Splits the large dataset into smaller chunks of `500,000` rows for efficient processing.\n",
    "\n",
    "4. **Read Shapefile (`ea_ghcf_icpac.shp`)**:\n",
    "   - Loads the geographic boundaries of East Africa into a GeoDataFrame (`ea_boundary`).\n",
    "\n",
    "5. **Spatial Join**:\n",
    "   - For each chunk of crop data:\n",
    "     - Converts longitude and latitude into geometry points (`gdb`).\n",
    "     - Performs a spatial join (`sjoin`) to associate each data point with its corresponding boundary region.\n",
    "     - Appends the results to a list (`edf_cont`).\n",
    "\n",
    "6. **Concatenate Processed Data**:\n",
    "   - Combines all processed chunks into a single DataFrame (`edf1`).\n",
    "   - Extracts the relevant columns (`latitude`, `longitude`, `value`) into a new DataFrame (`edf2`).\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose:\n",
    "Prepares geospatially-referenced crop production data for further analysis or modeling by linking crop data points to East African boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f3083d-e7ff-425e-bd47-ec83eb17a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import Exposures\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "#from Configuration import *\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "#file_path = lp_csv_files[5] # define the full file path of the CSV-file\n",
    "\n",
    "from more_itertools import sliced\n",
    "import geopandas as gp\n",
    "CHUNK_SIZE = 500000\n",
    "\n",
    "ex_db=pd.read_csv('ea_agr_spam.csv')\n",
    "\n",
    "index_slices = sliced(range(len(ex_db)), CHUNK_SIZE)\n",
    "\n",
    "ea_boundary=gp.read_file('shp_files/ea_ghcf_icpac.shp')\n",
    "\n",
    "edf_cont=[]\n",
    "for index_slice in index_slices:\n",
    "    chunk = ex_db.iloc[index_slice]\n",
    "    gdb = gp.GeoDataFrame(chunk, geometry=gp.points_from_xy(chunk.longitude, chunk.latitude))\n",
    "    edf=gp.sjoin(ea_boundary,gdb)\n",
    "    #edf1=edf[['GID_0', 'COUNTRY','gno','Nomotorway','primary','secondary','tertiary','unclassified','lon','lat', 'grid_name']]\n",
    "    edf_cont.append(edf)\n",
    "\n",
    "\n",
    "edf1=pd.concat(edf_cont)\n",
    "edf2=edf1[['latitude','longitude','value']]\n",
    "edf2\n",
    "\n",
    "#file_path='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_climada/KEN_2021.csv'\n",
    "#new_exp = Exposures(pd.read_csv(file_path))\n",
    "#new_exp.check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f342a0-188f-48ec-bc90-330bf1db4075",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exp = Exposures(edf2)\n",
    "new_exp.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce09014-ae09-450d-805d-1552c71895a9",
   "metadata": {},
   "source": [
    "1. **Set Logarithmic Color Normalization**:\n",
    "   ```python\n",
    "   norm = colors.LogNorm(vmin=500, vmax=4.0e8)\n",
    "   ```\n",
    "   - Configures a logarithmic scale for normalizing data values during plotting.\n",
    "   - Sets a minimum value (`vmin=500`) and a maximum value (`vmax=4.0e8`).\n",
    "   - This ensures that data with large ranges is visually distinguishable in the plot.\n",
    "\n",
    "2. **Plot Data as a Hexbin Map**:\n",
    "   ```python\n",
    "   ax = new_exp.plot_hexbin(norm=norm, pop_name=False, cmap='RdBu_r', buffer=1)\n",
    "   ```\n",
    "   - `new_exp`: An `Exposures` object containing geospatial exposure data.\n",
    "   - `plot_hexbin`: Plots the exposures as a hexbin map where hexagonal bins aggregate data points.\n",
    "     - `norm=norm`: Applies the logarithmic normalization defined earlier.\n",
    "     - `pop_name=False`: Disables population names in the plot.\n",
    "     - `cmap='RdBu_r'`: Sets the color map to a reversed red-to-blue gradient.\n",
    "     - `buffer=1`: Extends the plotting area slightly around the exposure data.\n",
    "\n",
    "3. **Save the Plot as an Image**:\n",
    "   ```python\n",
    "   plt.savefig('../../ea_agr_spam_v1.png', bbox_inches='tight')\n",
    "   ```\n",
    "   - Saves the generated hexbin plot to a file named `ea_agr_spam_v1.png` in the specified directory.\n",
    "   - `bbox_inches='tight'`: Ensures the image output tightly fits the plotted content.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose:\n",
    "The code creates a hexbin plot of geospatial exposure data from the `new_exp` object, using a logarithmic color scale for better visualization of data spanning large ranges. The plot is saved as a PNG image for further use or analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309d279-8e47-433b-8ebc-57f3cb3fecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "norm = colors.LogNorm(vmin=500, vmax=4.0e8)\n",
    "\n",
    "ax=new_exp.plot_hexbin(norm=norm, pop_name=False, cmap='RdBu_r', buffer=1)\n",
    "\n",
    "#fname='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_ibf_data_resources/exposure-data/gis/ea_global_background.shp'\n",
    "\n",
    "#ax.add_geometries(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='None')\n",
    "\n",
    "plt.savefig('ea_agr_spam_v1.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8dd15-2c3f-4a26-9170-edfaf2bfc46f",
   "metadata": {},
   "source": [
    "1. **Import Required Modules**:\n",
    "   - `Exposures` from `climada.entity`: Used to handle geospatial exposure data for impact analysis.\n",
    "   - Additional libraries like `pandas`, `numpy`, `matplotlib`, and `cartopy` are imported for data manipulation, visualization, and geospatial operations.\n",
    "\n",
    "2. **Define CSV File Path**:\n",
    "   ```python\n",
    "   file_path = '../../ea_agr_spam.csv'\n",
    "   ```\n",
    "   - Specifies the file path to the `ea_agr_spam.csv`, which contains exposure data such as geographic coordinates, crop values, and other relevant information.\n",
    "\n",
    "3. **Load the Exposure Data**:\n",
    "   ```python\n",
    "   new_exp = Exposures(pd.read_csv(file_path))\n",
    "   ```\n",
    "   - Reads the CSV file into a pandas DataFrame using `pd.read_csv(file_path)`.\n",
    "   - Converts the DataFrame into a `Exposures` object (`new_exp`), which is specifically designed for geospatial exposure data in CLIMADA.\n",
    "\n",
    "4. **Inspect the Loaded Exposure Object**:\n",
    "   ```python\n",
    "   new_exp\n",
    "   ```\n",
    "   - Outputs the `Exposures` object (`new_exp`) to examine its contents, such as the data structure, columns, and metadata.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose:\n",
    "The code reads exposure data (e.g., agricultural statistics from `ea_agr_spam.csv`) into a `Exposures` object. This object can then be used for further geospatial analysis or impact modeling in CLIMADA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427cab9-c467-450d-9ea0-e4ae821921bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import Exposures\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "#from Configuration import *\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "#file_path = lp_csv_files[5] # define the full file path of the CSV-file\n",
    "\n",
    "file_path='ea_agr_spam.csv'\n",
    "\n",
    "\n",
    "#file_path='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_climada/KEN_2021.csv'\n",
    "new_exp = Exposures(pd.read_csv(file_path))\n",
    "new_exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c209e-a237-4b25-bb71-310019f199f6",
   "metadata": {},
   "source": [
    "## hazard \n",
    "\n",
    "1. **Import Required Libraries**:\n",
    "   - `xarray`: For handling multi-dimensional labeled datasets.\n",
    "   - `rioxarray`: Extends xarray for geospatial raster operations.\n",
    "   - `pandas`: For working with time series and tabular data.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Define File Path for WRSI Data**:\n",
    "   ```python\n",
    "   wrsi_mean_path = '../../202411_wrsi/ens_mean_wrsi_2024_20241126_21.838949_51.415695_-11.745695_23.145147.nc'\n",
    "   ```\n",
    "   - Specifies the path to a NetCDF file containing WRSI (Water Requirement Satisfaction Index) data for November 2024.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Open the NetCDF Dataset**:\n",
    "   ```python\n",
    "   db1 = xr.open_dataset(wrsi_mean_path)\n",
    "   ```\n",
    "   - Reads the NetCDF file into an xarray `Dataset` object (`db1`), enabling efficient handling of multi-dimensional geospatial data.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Rename and Drop Variables**:\n",
    "   ```python\n",
    "   db1['spei'] = db1['__xarray_dataarray_variable__']\n",
    "   db2 = db1.drop(['__xarray_dataarray_variable__'])\n",
    "   ```\n",
    "   - `db1['spei']`: Renames the data variable `__xarray_dataarray_variable__` to a more meaningful name, `spei` (Standardized Precipitation-Evapotranspiration Index).\n",
    "   - `db1.drop()`: Drops the original variable `__xarray_dataarray_variable__` from the dataset, resulting in `db2`.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Rearrange Dimensions**:\n",
    "   ```python\n",
    "   db3 = db2.transpose('latitude', 'longitude')\n",
    "   ```\n",
    "   - Reorders the dimensions of the dataset to place `latitude` first and `longitude` second. This is a common format required by geospatial tools.\n",
    "\n",
    "---\n",
    "\n",
    "6. **Save as GeoTIFF**:\n",
    "   ```python\n",
    "   db3.rio.to_raster(f'../../wrsi_202412.tif', recalc_transform=False)\n",
    "   ```\n",
    "   - Converts the xarray dataset (`db3`) into a GeoTIFF file and saves it as `wrsi_202412.tif`.\n",
    "   - `recalc_transform=False`: Prevents recalculation of the coordinate transformation during the raster export.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose:\n",
    "This code processes a NetCDF file containing WRSI data by:\n",
    "1. Renaming variables for clarity.\n",
    "2. Reordering dimensions for geospatial compatibility.\n",
    "3. Saving the processed data as a GeoTIFF file (`wrsi_202412.tif`), which can be used in GIS software or further analysis.\n",
    "\n",
    "The workflow is tailored for geospatial data preparation and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27e5325-7496-4ed6-8515-c4884fad4c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray \n",
    "import pandas as pd\n",
    "\n",
    "wrsi_mean_path=f'202411_wrsi/ens_mean_wrsi_2024_20241126_21.838949_51.415695_-11.745695_23.145147.nc'\n",
    "\n",
    "db1=xr.open_dataset(wrsi_mean_path)\n",
    "#db1=db.rename({'longitude':'lon','latitude':'lat'})\n",
    "\n",
    "#times = pd.date_range(\"2024/12/\",\"2023/02/11\",freq='D')\n",
    "\n",
    "db1['spei'] = db1['__xarray_dataarray_variable__']\n",
    "db2 = db1.drop(['__xarray_dataarray_variable__'])\n",
    "\n",
    "#db3=db2.transpose('lat', 'lon')\n",
    "\n",
    "#db3=db2.spei.cf\n",
    "\n",
    "db3=db2.transpose( 'latitude', 'longitude')\n",
    "\n",
    "db3.rio.to_raster(f'../../wrsi_202412.tif',recalc_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a469a5-f3e6-43c3-a1a9-35d50df753ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from climada.hazard import Hazard\n",
    "\n",
    "haz_ven = Hazard.from_raster([f'wrsi_202412.tif'], dst_crs='epsg:4326',attrs={'frequency':np.ones(1)/2}, haz_type='DR')\n",
    "haz_ven.check()\n",
    "print('\\n Solution 1:')\n",
    "print('centroids CRS:', haz_ven.centroids.crs)\n",
    "print('raster info:', haz_ven.centroids.meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde60294-1884-4581-8556-31b0f86d1773",
   "metadata": {},
   "source": [
    "## Impact calcualtion\n",
    "\n",
    "1. **Import Impact Calculation Module**:\n",
    "   ```python\n",
    "   from climada.engine import ImpactCalc\n",
    "   ```\n",
    "   - `ImpactCalc`: A class from CLIMADA used to compute the impact of hazards on exposures using impact functions.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Assign an Impact Function to Exposures**:\n",
    "   ```python\n",
    "   impact_func_id = 1  # Define or load an appropriate impact function ID\n",
    "   new_exp.gdf[f'impf_DR'] = impact_func_id\n",
    "   ```\n",
    "   - Assigns an impact function ID (`1`) to exposures in `new_exp`. \n",
    "   - The column `impf_DR` specifies the impact function for the `DR` hazard type (e.g., drought). \n",
    "   - This links each exposure to the appropriate impact function for the hazard.\n",
    "\n",
    "---\n",
    "\n",
    "3. **Perform Impact Calculation**:\n",
    "   ```python\n",
    "   imp_calc = ImpactCalc(new_exp, imp_set_xlsx, haz_ven)\n",
    "   result = imp_calc.impact()\n",
    "   ```\n",
    "   - `ImpactCalc(new_exp, imp_set_xlsx, haz_ven)`:\n",
    "     - `new_exp`: The exposure data (e.g., crops or infrastructure) prepared with geospatial references.\n",
    "     - `imp_set_xlsx`: The impact functions loaded from an Excel file.\n",
    "     - `haz_ven`: The hazard data (e.g., drought intensity and frequency).\n",
    "   - `result = imp_calc.impact()`:\n",
    "     - Computes the impact of the hazard (`haz_ven`) on the exposures (`new_exp`) using the assigned impact functions.\n",
    "     - Returns the impact result, which quantifies the expected damage or loss.\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose:\n",
    "This code calculates the impact of a specific hazard (e.g., drought) on exposures (e.g., agricultural areas) using pre-defined impact functions. The result provides a quantitative measure of the expected damage, facilitating risk assessment and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7837e805-5ff0-4d90-bdab-412ac0bc96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.engine import ImpactCalc\n",
    "\n",
    "#haz_type = haz_ven.haz_type\n",
    "impact_func_id = 1  # Define or load an appropriate impact function ID\n",
    "\n",
    "# Use the gdf attribute to access and modify data\n",
    "new_exp.gdf[f'impf_DR'] = impact_func_id\n",
    "#imp_set = ImpactFuncSet.from_dict(imp_set_xlsx)\n",
    "\n",
    "# Calculate the impact\n",
    "imp_calc = ImpactCalc(new_exp, imp_set_xlsx, haz_ven)\n",
    "result = imp_calc.impact()  # Computes the impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4f86e-2ebf-4f19-8bd7-4ad248e75bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exp.gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6ee632-6b58-48a9-a2c3-3b09d256660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_event_start = result.event_name.index('1')\n",
    "damages_drought = np.asarray([result.at_event[index_event_start]])\n",
    "print(damages_drought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a373caa8-7970-47c4-a857-80766b0ec776",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.plot_scatter_eai_exposure(pop_name=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a4b7b6-9f10-4fed-980d-4bdef915ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.write_csv('../../impact_202412.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dadd074-b2ad-4ac9-8991-ea9418112804",
   "metadata": {},
   "source": [
    "## Probablity maps \n",
    "\n",
    "1. **Read and Prepare Impact Data**:\n",
    "   - Load `impact_202412.csv` and extract impact-related columns (`eai_exp`, `exp_lat`, `exp_lon`).\n",
    "   - Convert exposure data into a GeoDataFrame (`g_db1`) with point geometries.\n",
    "\n",
    "2. **Classify Exposure Values**:\n",
    "   - Classify `eai_exp` into four bins (\"Low\", \"Medium-Low\", etc.) and add the classifications to the DataFrame (`class` column).\n",
    "\n",
    "3. **Read WRSI Data and Convert to GeoDataFrame**:\n",
    "   - Open a NetCDF file (`prob_lower_tercile_2024...nc`) containing WRSI probabilities and convert it into a GeoDataFrame (`gdf1`).\n",
    "   - Generate polygon buffers around points in `gdf1` to create geometries for spatial joins.\n",
    "\n",
    "4. **Spatial Join for Exposure and WRSI**:\n",
    "   - Perform a spatial join (`sjoin`) between exposures (`g_db1`) and WRSI probabilities (`gdf3`) to associate each exposure point with the WRSI data.\n",
    "\n",
    "5. **Apply Probability-Based Classification**:\n",
    "   - Define a function (`get_prob_ibf`) that assigns impact-based classifications (`ibf`) based on `prob_wrsi` and `eai_exp` ranges.\n",
    "   - Apply this function to the joined dataset to calculate the `ibf` value for each exposure point.\n",
    "\n",
    "6. **Prepare Final Output**:\n",
    "   - Extract relevant columns (`latitude`, `longitude`, `ibf`) into a new DataFrame (`wsd1`).\n",
    "   - Add a `region_id` column and save the processed data as a CSV file (`probablity_ibf_output_d.csv`).\n",
    "\n",
    "---\n",
    "\n",
    "### Purpose:\n",
    "This code integrates exposure data (`eai_exp`) and WRSI probabilities to calculate and classify the probabilistic impact-based forecast (`ibf`). It generates a geospatial dataset ready for visualization or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9200b21b-37fe-44d6-80a6-e7a1a4d569cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "\n",
    "db=pd.read_csv('../../impact_202412.csv')\n",
    "db1=db[['eai_exp','exp_lat','exp_lon']]\n",
    "db1.info()\n",
    "g_db1 = gp.GeoDataFrame(db1, geometry=gp.points_from_xy(db1.exp_lon, db1.exp_lat))\n",
    "g_db1.set_geometry(\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba57a0d-1400-400c-8db2-880b86135134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify into four bins from min to max\n",
    "cuts, bin_edges = pd.cut(db1['eai_exp'], bins=4, retbins=True, labels=[\"Low\", \"Medium-Low\", \"Medium-High\", \"High\"], right=True)\n",
    "db1['class'] = cuts\n",
    "\n",
    "# Printing the DataFrame with classes\n",
    "print(db1)\n",
    "\n",
    "# Printing the bin edges\n",
    "print(\"Bin edges:\", bin_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d150d01-51ff-43d1-ad22-47b244e78b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath=f'../../202411_wrsi/prob_lower_tercile_2024_20241126_21.838949_51.415695_-11.745695_23.145147.nc'\n",
    "\n",
    "db1=xr.open_dataset(dbpath)\n",
    "\n",
    "erf=db1.to_dataframe()\n",
    "\n",
    "erf1=erf.reset_index()\n",
    "\n",
    "gdf1 = gp.GeoDataFrame(erf1, geometry=gp.points_from_xy(erf1.longitude, erf1.latitude))\n",
    "\n",
    "#gdf1=gdf[0:12]\n",
    "\n",
    "gdf1['polygon']=gdf1.geometry.apply(lambda g: g.buffer(0.125, cap_style=3))\n",
    "\n",
    "gdf2=gdf1[['__xarray_dataarray_variable__','polygon']]\n",
    "gdf2.columns=['prob_wrsi','geometry']\n",
    "#gdf1\n",
    "gdf3=gdf2.set_geometry(\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f591a3bb-31ab-49ed-af68-cad716f6aa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd=g_db1.sjoin(gdf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c44b1-b4c2-4966-a17d-d4a64bb45bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_prob_ibf(row):\n",
    "    #print(row['prob_wrsi'],row['eai_exp'])\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 0<= row['eai_exp'] <= 3166627.35:\n",
    "        a=10    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 0<= row['eai_exp'] <= 3166627.35:\n",
    "        a=10    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 0<= row['eai_exp'] <= 3166627.35:\n",
    "        a=10    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 0<= row['eai_exp'] <= 3166627.35:\n",
    "        a=10    \n",
    "    ########\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 3166627.35<= row['eai_exp'] <= 6333254.7:\n",
    "        a=10    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 3166627.35<= row['eai_exp'] <= 6333254.7:\n",
    "        a=10    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 3166627.35<= row['eai_exp'] <= 6333254.7:\n",
    "        a=20    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 3166627.35<= row['eai_exp'] <= 6333254.7:\n",
    "        a=20    \n",
    "    ########\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 6333254.7<= row['eai_exp'] <=9499882.05:\n",
    "        a=20    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 6333254.7<= row['eai_exp'] <= 9499882.05:\n",
    "        a=20    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 6333254.7<= row['eai_exp'] <= 9499882.05:\n",
    "        a=30    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 6333254.7<= row['eai_exp'] <= 9499882.05:\n",
    "        a=30    \n",
    "    ########\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 9499882.05<= row['eai_exp'] <=12666509.4:\n",
    "        a=20    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 9499882.05<= row['eai_exp'] <=12666509.4:\n",
    "        a=30    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 9499882.05<= row['eai_exp'] <= 12666509.4:\n",
    "        a=30    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 9499882.05<= row['eai_exp'] <= 12666509.4:\n",
    "        a=40\n",
    "    ########\n",
    "    if row['prob_wrsi'] is None and row['eai_exp'] is None: \n",
    "        a=np.nan\n",
    "    if row['prob_wrsi'] is None or row['eai_exp'] is None: \n",
    "        a=np.nan \n",
    "    if pd.isna(row['prob_wrsi']):\n",
    "        a=np.nan\n",
    "    return a\n",
    "\n",
    "wsd['ibf']=wsd.apply(lambda row: get_prob_ibf(row), axis = 1)\n",
    "wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7d49f3-dfe6-4e59-ad91-5c39acf69b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd1=wsd[['exp_lat','exp_lon','ibf']]\n",
    "wsd1.columns=['latitude','longitude','value']\n",
    "\n",
    "wsd1['region_id']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182efbf-4932-436e-868e-e69ff8cf926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd1.to_csv('../../probablity_ibf_output_d.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed824553-ddb0-432d-99df-80bb71f17fb9",
   "metadata": {},
   "source": [
    "## maping of prob_ibf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fb237b-671d-419c-8e98-0c897eb354c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb=pd.read_csv('../../probablity_ibf_output_d.csv')\n",
    "\n",
    "from more_itertools import sliced\n",
    "import geopandas as gp\n",
    "CHUNK_SIZE = 500000\n",
    "\n",
    "index_slices = sliced(range(len(db)), CHUNK_SIZE)\n",
    "\n",
    "ea_boundary=gp.read_file('../../shp_files/ea_ghcf_icpac.shp')\n",
    "\n",
    "edf_cont=[]\n",
    "for index_slice in index_slices:\n",
    "    chunk = pdb.iloc[index_slice]\n",
    "    gdb = gp.GeoDataFrame(chunk, geometry=gp.points_from_xy(chunk.longitude, chunk.latitude))\n",
    "    edf=gp.sjoin(ea_boundary,gdb)\n",
    "    #edf1=edf[['GID_0', 'COUNTRY','gno','Nomotorway','primary','secondary','tertiary','unclassified','lon','lat', 'grid_name']]\n",
    "    edf_cont.append(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f2a991-b590-445c-9d81-cd5090c37f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf1=pd.concat(edf_cont)\n",
    "edf2=edf1[['latitude','longitude','value']]\n",
    "edf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11621c-71c3-4638-996e-8678743be7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab016e9-c1d4-4705-bec3-0785f108fd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = edf2.groupby('value').count()\n",
    "count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8821ce-8f2c-4caa-9581-782e63a82ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import Exposures\n",
    "import matplotlib\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "#from Configuration import *\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "#file_path = lp_csv_files[5] # define the full file path of the CSV-file\n",
    "\n",
    "def return_colormap():\n",
    "    \"\"\"\n",
    "    Create colormap of matplotlib based on number of class and given colorcode\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : class object\n",
    "        Input/Output parameter definitions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    c_cmap : Object\n",
    "        matplotlib colormap.\n",
    "\n",
    "    \"\"\"\n",
    "    c = matplotlib.colors.ColorConverter().to_rgb\n",
    "    colorlist=[c(\"#00c252\"), c(\"#f3ff00\"), c(\"#c85500\"), c(\"#ff0000\")]\n",
    "    color_code=colorlist\n",
    "    classif= [10, 20, 30, 40]\n",
    "    c_cmap = LinearSegmentedColormap.from_list(\"my_colormap\",color_code, N=len(classif), gamma=1.0)\n",
    "    return c_cmap\n",
    "\n",
    "\n",
    "\n",
    "#file_path='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_climada/KEN_2021.csv'\n",
    "new_exp = Exposures(edf2)\n",
    "new_exp.check()\n",
    "\n",
    "norm = colors.LogNorm(vmin=10, vmax=40)\n",
    "\n",
    "c_cmap=return_colormap()\n",
    "\n",
    "ax=new_exp.plot_hexbin(norm=norm, pop_name=False, cmap=c_cmap, buffer=1)\n",
    "\n",
    "#fname='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_ibf_data_resources/exposure-data/gis/ea_global_background.shp'\n",
    "\n",
    "#ax.add_geometries(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='None')\n",
    "\n",
    "#plt.savefig('/home/ibf.png', bbox_inches='tight')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
