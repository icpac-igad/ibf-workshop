{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b9a309e",
   "metadata": {},
   "source": [
    "## impact function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ee19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import ImpactFuncSet\n",
    "from climada.util import ENT_TEMPLATE_XLS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# provide absolute path of the input excel file\n",
    "#file_name = ENT_TEMPLATE_XLS\n",
    "file_name=f'/srv/repo/IBF_workshop_data/wrsi_data/ibf_drought_impact_ea_v0.xlsx'\n",
    "\n",
    "imp_set_xlsx = ImpactFuncSet.from_excel(file_name)\n",
    "\n",
    "imp_set_xlsx.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cd79d3",
   "metadata": {},
   "source": [
    "## exposure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import Exposures\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "#from Configuration import *\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "#file_path = lp_csv_files[5] # define the full file path of the CSV-file\n",
    "\n",
    "from more_itertools import sliced\n",
    "import geopandas as gp\n",
    "CHUNK_SIZE = 500000\n",
    "\n",
    "ex_db=pd.read_csv('/srv/repo/IBF_workshop_data/wrsi_data/ea_agr_spam.csv')\n",
    "\n",
    "index_slices = sliced(range(len(ex_db)), CHUNK_SIZE)\n",
    "\n",
    "ea_boundary=gp.read_file('/srv/repo/IBF_workshop_data/gis_files/ea_ghcf_icpac.shp')\n",
    "\n",
    "edf_cont=[]\n",
    "for index_slice in index_slices:\n",
    "    chunk = ex_db.iloc[index_slice]\n",
    "    gdb = gp.GeoDataFrame(chunk, geometry=gp.points_from_xy(chunk.longitude, chunk.latitude))\n",
    "    edf=gp.sjoin(ea_boundary,gdb)\n",
    "    #edf1=edf[['GID_0', 'COUNTRY','gno','Nomotorway','primary','secondary','tertiary','unclassified','lon','lat', 'grid_name']]\n",
    "    edf_cont.append(edf)\n",
    "\n",
    "\n",
    "edf1=pd.concat(edf_cont)\n",
    "edf2=edf1[['latitude','longitude','value']]\n",
    "#edf2\n",
    "\n",
    "#file_path='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_climada/KEN_2021.csv'\n",
    "#new_exp = Exposures(pd.read_csv(file_path))\n",
    "#new_exp.check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8987419",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_exp = Exposures(edf2)\n",
    "new_exp.check()\n",
    "\n",
    "norm = colors.LogNorm(vmin=500, vmax=4.0e8)\n",
    "\n",
    "ax=new_exp.plot_hexbin(norm=norm, pop_name=False, cmap='RdBu_r', buffer=1)\n",
    "\n",
    "#fname='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_ibf_data_resources/exposure-data/gis/ea_global_background.shp'\n",
    "\n",
    "#ax.add_geometries(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='None')\n",
    "\n",
    "plt.savefig('/srv/repo/IBF_workshop_data/wrsi_data/ea_agr_spam_v1.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125a2fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import Exposures\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "#from Configuration import *\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "#file_path = lp_csv_files[5] # define the full file path of the CSV-file\n",
    "\n",
    "file_path='/srv/repo/IBF_workshop_data/wrsi_data/ea_agr_spam.csv'\n",
    "\n",
    "\n",
    "#file_path='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_climada/KEN_2021.csv'\n",
    "new_exp = Exposures(pd.read_csv(file_path))\n",
    "new_exp.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45308bd",
   "metadata": {},
   "source": [
    "## hazard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c193615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray \n",
    "import pandas as pd\n",
    "\n",
    "wrsi_mean_path=f'/srv/repo/IBF_workshop_data/wrsi_data/ens_mean_wrsi_MAM2023_20230211_21.838949_51.415695_-11.745695_23.145147.nc'\n",
    "\n",
    "db1=xr.open_dataset(wrsi_mean_path)\n",
    "#db1=db.rename({'longitude':'lon','latitude':'lat'})\n",
    "\n",
    "times = pd.date_range(\"2023/02/11\",\"2023/02/11\",freq='D')\n",
    "\n",
    "db1['spei'] = db1['__xarray_dataarray_variable__']\n",
    "db2 = db1.drop(['__xarray_dataarray_variable__'])\n",
    "\n",
    "#db3=db2.transpose('lat', 'lon')\n",
    "\n",
    "#db3=db2.spei.cf\n",
    "\n",
    "db3=db2.transpose( 'latitude', 'longitude')\n",
    "\n",
    "db3.rio.to_raster(f'/srv/repo/IBF_workshop_data/wrsi_data/wrsi_20230211.tif',recalc_transform=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1fa7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from climada.hazard import Hazard\n",
    "\n",
    "haz_ven = Hazard.from_raster([f'/srv/repo/IBF_workshop_data/wrsi_data/wrsi_20230211.tif'], dst_crs='epsg:4326',attrs={'frequency':np.ones(1)/2}, haz_type='DR')\n",
    "haz_ven.check()\n",
    "print('\\n Solution 1:')\n",
    "print('centroids CRS:', haz_ven.centroids.crs)\n",
    "print('raster info:', haz_ven.centroids.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976cfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.engine import Impact\n",
    "\n",
    "imp_drought = Impact()\n",
    "\n",
    "\"\"\"Calculate Damage for a specific event\"\"\"\n",
    "imp_drought.calc(new_exp, imp_set_xlsx, haz_ven)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d402bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_event_start = imp_drought.event_name.index('1')\n",
    "damages_drought = np.asarray([imp_drought.at_event[index_event_start]])\n",
    "print(damages_drought)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4483929",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_drought.plot_scatter_eai_exposure(pop_name=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "038a79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_drought.write_csv('/srv/repo/IBF_workshop_data/wrsi_data/impact_20230211.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf2bb166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_46/1055947553.py:3: DtypeWarning: Columns (0,1,2,3,10,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  new_imp = Impact(pd.read_csv('/srv/repo/IBF_workshop_data/wrsi_data/impact_20230211.csv'))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Hazard event ids 40807 and event names 0 are not of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclimada\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Impact\n\u001b[0;32m----> 3\u001b[0m new_imp \u001b[38;5;241m=\u001b[39m \u001b[43mImpact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/srv/repo/IBF_workshop_data/wrsi_data/impact_20230211.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#new_imp = imp_drought(pd.read_csv('/home/20230211/impact_20230211.csv'))\u001b[39;00m\n\u001b[1;32m      6\u001b[0m new_imp\u001b[38;5;241m.\u001b[39mcheck()\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/climada/engine/impact.py:162\u001b[0m, in \u001b[0;36mImpact.__init__\u001b[0;34m(self, event_id, event_name, date, frequency, frequency_unit, coord_exp, crs, eai_exp, at_event, tot_value, aai_agg, unit, imp_mat, tag)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit \u001b[38;5;241m=\u001b[39m unit\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_id) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_name):\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHazard event ids \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and event names\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_name)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not of the same length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_id) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHazard event ids \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevent_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and event dates\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdate)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are not of the same length\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: Hazard event ids 40807 and event names 0 are not of the same length"
     ]
    }
   ],
   "source": [
    "from climada.engine import Impact\n",
    "\n",
    "aa=pd.read_csv('/srv/repo/IBF_workshop_data/wrsi_data/impact_20230211.csv'\n",
    "\n",
    "#new_imp = imp_drought(pd.read_csv('/home/20230211/impact_20230211.csv'))\n",
    "new_imp.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbedf7cb",
   "metadata": {},
   "source": [
    "## Probablity maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47129a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "\n",
    "db=pd.read_csv('/home/20230211/impact_20230211.csv')\n",
    "db1=db[['eai_exp','exp_lat','exp_lon']]\n",
    "db1\n",
    "\n",
    "g_db1 = gp.GeoDataFrame(db1, geometry=gp.points_from_xy(db1.exp_lon, db1.exp_lat))\n",
    "g_db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf07a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpath=f'/home/20230211/prob_lower_tercile_MAM2023_20230211_21.838949_51.415695_-11.745695_23.145147.nc'\n",
    "\n",
    "db1=xr.open_dataset(dbpath)\n",
    "\n",
    "erf=db1.to_dataframe()\n",
    "\n",
    "erf1=erf.reset_index()\n",
    "\n",
    "gdf1 = gp.GeoDataFrame(erf1, geometry=gp.points_from_xy(erf1.longitude, erf1.latitude))\n",
    "\n",
    "#gdf1=gdf[0:12]\n",
    "\n",
    "gdf1['polygon']=gdf1.geometry.apply(lambda g: g.buffer(0.125, cap_style=3))\n",
    "\n",
    "gdf2=gdf1[['__xarray_dataarray_variable__','polygon']]\n",
    "gdf2.columns=['prob_wrsi','geometry']\n",
    "#gdf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cfa7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd=gdf2.sjoin(g_db1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_prob_ibf(row):\n",
    "    #print(row['prob_wrsi'],row['eai_exp'])\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 0.0<= row['eai_exp'] <= 68716.9:\n",
    "        a=10    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 0.0<= row['eai_exp'] <= 68716.9:\n",
    "        a=10    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 0.0<= row['eai_exp'] <= 68716.9:\n",
    "        a=10    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 0.0<= row['eai_exp'] <= 68716.9:\n",
    "        a=10    \n",
    "    ########\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 68716.9<= row['eai_exp'] <= 687169:\n",
    "        a=10    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 68716.9<= row['eai_exp'] <= 687169:\n",
    "        a=10    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 68716.9<= row['eai_exp'] <= 687169:\n",
    "        a=20    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 68716.9<= row['eai_exp'] <= 687169:\n",
    "        a=20    \n",
    "    ########\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 687169<= row['eai_exp'] <=296190.70:\n",
    "        a=20    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 687169<= row['eai_exp'] <= 296190.70:\n",
    "        a=20    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 687169<= row['eai_exp'] <= 296190.70:\n",
    "        a=30    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 687169<= row['eai_exp'] <= 296190.70:\n",
    "        a=30    \n",
    "    ########\n",
    "    if 0.0<= row['prob_wrsi'] <=0.25 and 296190.70<= row['eai_exp'] <=29619070:\n",
    "        a=20    \n",
    "    if 0.25<= row['prob_wrsi'] <=0.5 and 296190.70<= row['eai_exp'] <= 29619070:\n",
    "        a=30    \n",
    "    if 0.5<= row['prob_wrsi'] <=0.75 and 296190.70<= row['eai_exp'] <= 29619070:\n",
    "        a=30    \n",
    "    if 0.75<= row['prob_wrsi'] <=1 and 296190.70<= row['eai_exp'] <= 29619070:\n",
    "        a=40\n",
    "    ########\n",
    "    if row['prob_wrsi'] is None and row['eai_exp'] is None: \n",
    "        a=np.nan\n",
    "    if row['prob_wrsi'] is None or row['eai_exp'] is None: \n",
    "        a=np.nan \n",
    "    if pd.isna(row['prob_wrsi']):\n",
    "        a=np.nan\n",
    "    return a\n",
    "\n",
    "wsd['ibf']=wsd.apply(lambda row: get_prob_ibf(row), axis = 1)\n",
    "wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177df560",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd1=wsd[['exp_lat','exp_lon','ibf']]\n",
    "wsd1.columns=['latitude','longitude','value']\n",
    "\n",
    "wsd1['region_id']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wsd1.to_csv('/home/20230211/probablity_ibf_output.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ea330d",
   "metadata": {},
   "source": [
    "## maping of prob_ibf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2caf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "db=pd.read_csv('/home/20230211/probablity_ibf_output.csv')\n",
    "\n",
    "from more_itertools import sliced\n",
    "import geopandas as gp\n",
    "CHUNK_SIZE = 500000\n",
    "\n",
    "index_slices = sliced(range(len(db)), CHUNK_SIZE)\n",
    "\n",
    "ea_boundary=gp.read_file('/home/ea_shapefiles/ea_ghcf_icpac.shp')\n",
    "\n",
    "edf_cont=[]\n",
    "for index_slice in index_slices:\n",
    "    chunk = db.iloc[index_slice]\n",
    "    gdb = gp.GeoDataFrame(chunk, geometry=gp.points_from_xy(chunk.longitude, chunk.latitude))\n",
    "    edf=gp.sjoin(ea_boundary,gdb)\n",
    "    #edf1=edf[['GID_0', 'COUNTRY','gno','Nomotorway','primary','secondary','tertiary','unclassified','lon','lat', 'grid_name']]\n",
    "    edf_cont.append(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e3280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf1=pd.concat(edf_cont)\n",
    "edf2=edf1[['latitude','longitude','value']]\n",
    "edf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3fe250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from climada.entity import Exposures\n",
    "import matplotlib\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import colors\n",
    "from matplotlib import pyplot as plt\n",
    "#from Configuration import *\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "\n",
    "from cartopy.io.shapereader import Reader\n",
    "from cartopy.feature import ShapelyFeature\n",
    "\n",
    "#file_path = lp_csv_files[5] # define the full file path of the CSV-file\n",
    "\n",
    "def return_colormap():\n",
    "    \"\"\"\n",
    "    Create colormap of matplotlib based on number of class and given colorcode\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : class object\n",
    "        Input/Output parameter definitions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    c_cmap : Object\n",
    "        matplotlib colormap.\n",
    "\n",
    "    \"\"\"\n",
    "    c = matplotlib.colors.ColorConverter().to_rgb\n",
    "    colorlist=[c(\"#00c252\"), c(\"#f3ff00\"), c(\"#c85500\"), c(\"#ff0000\")]\n",
    "    color_code=colorlist\n",
    "    classif= [10, 20, 30, 40]\n",
    "    c_cmap = LinearSegmentedColormap.from_list(\"my_colormap\",color_code, N=len(classif), gamma=1.0)\n",
    "    return c_cmap\n",
    "\n",
    "\n",
    "\n",
    "#file_path='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_climada/KEN_2021.csv'\n",
    "new_exp = Exposures(edf2)\n",
    "new_exp.check()\n",
    "\n",
    "norm = colors.LogNorm(vmin=10, vmax=40)\n",
    "\n",
    "c_cmap=return_colormap()\n",
    "\n",
    "ax=new_exp.plot_hexbin(norm=norm, pop_name=False, cmap=c_cmap, buffer=1)\n",
    "\n",
    "#fname='/home/bulbul/Documents/07-2022/impact_weather_icpac/lab/ea_ibf_data_resources/exposure-data/gis/ea_global_background.shp'\n",
    "\n",
    "#ax.add_geometries(Reader(fname).geometries(),ccrs.PlateCarree(),facecolor='None')\n",
    "\n",
    "#plt.savefig('/home/ibf.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e479b",
   "metadata": {},
   "source": [
    "## country wise damage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "\n",
    "from more_itertools import sliced\n",
    "\n",
    "db=pd.read_csv('/home/20230211/impact_20230211.csv')\n",
    "db1=db[['eai_exp','exp_lat','exp_lon']]\n",
    "\n",
    "\n",
    "ea_boundary=gp.read_file('/home/ea_shapefiles/ea_ghcf_icpac.shp')\n",
    "\n",
    "CHUNK_SIZE = 500000\n",
    "\n",
    "index_slices = sliced(range(len(db1)), CHUNK_SIZE)\n",
    "\n",
    "edf_cont=[]\n",
    "for index_slice in index_slices:\n",
    "    chunk = db.iloc[index_slice]\n",
    "    gdb = gp.GeoDataFrame(chunk, geometry=gp.points_from_xy(chunk.exp_lon, chunk.exp_lat))\n",
    "    edf=gp.sjoin(ea_boundary,gdb)\n",
    "    #edf1=edf[['GID_0', 'COUNTRY','gno','Nomotorway','primary','secondary','tertiary','unclassified','lon','lat', 'grid_name']]\n",
    "    edf_cont.append(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94531306",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf1=pd.concat(edf_cont)\n",
    "edf1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2460e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf2=edf1[['GID_0', 'COUNTRY', 'eai_exp']]\n",
    "\n",
    "edf3=edf2.groupby(['GID_0']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edf4=edf3.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0abd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exdb=pd.read_csv('/home/ea_agr_spam.csv')\n",
    "\n",
    "CHUNK_SIZE = 500000\n",
    "\n",
    "index_slices = sliced(range(len(exdb)), CHUNK_SIZE)\n",
    "\n",
    "ex_edf_cont=[]\n",
    "for index_slice in index_slices:\n",
    "    chunk = exdb.iloc[index_slice]\n",
    "    gdb = gp.GeoDataFrame(chunk, geometry=gp.points_from_xy(chunk.longitude, chunk.latitude))\n",
    "    edf=gp.sjoin(ea_boundary,gdb)\n",
    "    #edf1=edf[['GID_0', 'COUNTRY','gno','Nomotorway','primary','secondary','tertiary','unclassified','lon','lat', 'grid_name']]\n",
    "    ex_edf_cont.append(edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_edf1=pd.concat(ex_edf_cont)\n",
    "ex_edf2=ex_edf1[['GID_0', 'COUNTRY', 'value']]\n",
    "\n",
    "ex_edf3=ex_edf2.groupby(['GID_0']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86667e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_edf4=ex_edf3.reset_index()\n",
    "ex_edf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e699c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "er=pd.merge(ex_edf4,edf4,on='GID_0')\n",
    "\n",
    "def percentage_change(col1,col2):\n",
    "    return ((col2 - col1) / col1) * 100\n",
    "\n",
    "er['per_loss'] =  percentage_change(er['value'],er['eai_exp'])\n",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8b182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "er.style.format(precision=2)\n",
    "\n",
    "er['Crop Production value in US$'] = '$' + (er['value'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "er['Crop Production value after impact factor in US$'] = '$' + (er['eai_exp'].astype(float)/1000000).round(2).astype(str) + 'MM'\n",
    "er1=er[['GID_0','Crop Production value in US$','Crop Production value after impact factor in US$','per_loss']]\n",
    "\n",
    "er1['Country']=['Burundi',\n",
    " 'Djibouti',\n",
    " 'Eritrea',\n",
    " 'Ethiopia',\n",
    " 'Kenya',\n",
    " 'Rwanda',\n",
    " 'Sudan',\n",
    " 'Somalia',\n",
    " 'South Sudan',\n",
    " 'Tanzania',\n",
    " 'Uganda']\n",
    "\n",
    "er2=er1[['Country','Crop Production value in US$','Crop Production value after impact factor in US$','per_loss']]\n",
    "er2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46d7fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "er1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32451bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_edf3=ex_edf2.groupby(['COUNTRY']).sum()\n",
    "ex_edf4=ex_edf3.reset_index()\n",
    "ex_edf4['COUNTRY'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924809a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
